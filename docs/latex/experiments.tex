\section{Experiments}

\subsection{Data Sets}
Data sets used in the paper were WSJ Treebank for POS tagging and CoNLL 2003
for the NER task. I have only used a portion of the Treebank because
the full version requires a special licence. The NLTK data package includes a
10\% sample of the Treebank. NER data set was used in its full form.
Although WSJ and CoNLL have pre-determined splits for the training,
development, and test sets, I have decided to perform custom splitting.
Data sets were split on a sentence level using the 60 - 20 - 20 ratio
and random shuffling was applied before the split. Table \ref{tab:data} shows
the number of examples in each of splits.

\begin{center}
\begin{tabular}{ r|c|c| }
\multicolumn{1}{r}{}
 &  \multicolumn{1}{c}{WSJ Treebank NLTK sample}
 & \multicolumn{1}{c}{ConLL 2003 NER} \\
\cline{2-3}
Train & 2348 & 12446 \\
\cline{2-3}
Development & 783 & 4149 \\
\cline{2-3}
Test & 783 & 4149 \\
\cline{2-3}
\end{tabular}
\captionof{table}{Number of examples (sentences) in dataset splits}
\label{tab:data}
\end{center}


\textbf{WSJ Penn Treebank}
Data set consists of 2499 selected stories from a three year Wall Street
Journal (WSJ) collection of 98732 stories for syntactic annotation. There are
45 unique POS tags including the $\text{-NONE-}$ tag for the unknown word tags. 

\textbf{CoNLL 2003 NER}
The shared task of CoNLL 2003 concerns language-independent named entity
recognition. Four types of entities existing in the data set are: persons,
locations, organizations and names of miscellaneous entities that do not belong
to the previous three groups. My experiments were focused on the English
version of the dataset although German data is also available.

\subsection{Results}
All experiment shared the same hyper parameters which are shown in Table
\ref{tab:hyper} If the hyper parameter is not present in the network
architecture e.g. Dropout in the Section \ref{no_dropout}, then the parameter
can be ignored.

\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\textbf{Layer} & \textbf{Hyper Parameter} & \textbf{POS} & \textbf{NER} \\ \hline
\multirow{2}{*}{CNN} & window size & 3 & 3 \\
 & number of filters & 30 & 30 \\ \hline
\multirow{3}{*}{LSTM} & state size & 200 & 200 \\
 & initial state & 0.0 & 0.0 \\
 & peepholes & no & no \\ \hline
Dropout & dropout rate & 0.5 & 0.5 \\ \hline
\multirow{2}{*}{} & batch size & 10 & 10\\
 & initial learning rate & 0.01 & 0.015 \\
 & decay rate & 0.05 & 0.05 \\
 & training epochs & 50 & 50 \\
\hline
\end{tabular}
\captionof{table}{Per layer hyper parameters for datasets}
\label{tab:hyper}
\end{center}

\label{no_dropout}
\subsubsection{With Dropout}
First set of experiments were conducted with dropout layers. Results after 50
epochs of training are displayed in the Table \ref{tab:dropout}

\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
&  & {\textbf{POS}} & {\textbf{NER}}\\ \hline
\multirow{4}{*}{Development} & Accuracy & 96.73 & \textbf{98.80} \\
 & Precision & 93.77 & 93.73 \\
 & Recall & 93.91 & 94.02 \\
 & F1 & 93.52 & 93.56 \\ \hline
\multirow{4}{*}{Test} & Accuracy & 96.71 & 98.23 \\
 & Precision & 93.73 & 91.45 \\
 & Recall & 93.80 & 91.90 \\
 & F1 & 93.45 & 91.30 \\ \hline
\end{tabular}
\captionof{table}{Results with dropout layers}
\label{tab:dropout}
\end{center}


\label{peculiar_dropout}
\subsubsection{No Dropout}
Table \ref{tab:no_dropout} shows the results when dropout layers were removed.
In comparison with dropout layers we can see the slight improvement in all
metrics across both data sets. Dropout technique usually gives better results
on the unseen examples but the explanation could be that my data was shuffled
on a sentence level across the whole corpus whereas authors have used first $n$
articles/sections for training and the rest for development and testing. It
could be that the few last articles/sections differ slightly in context from
the rest, so a network with the applied dropout would actually perform better.
On the contrary, my model has seen examples from the last few articles/sections so it
performs better when there is no regularization applied in the form of dropout.

\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
& & {\textbf{POS}} & {\textbf{NER}}\\ \hline
\multirow{4}{*}{Development} & Accuracy & \textbf{97.08} & 98.78 \\
 & Precision & \textbf{94.24} & \textbf{93.69} \\
 & Recall & \textbf{94.28} & \textbf{94.13} \\
 & F1 & \textbf{93.97} & \textbf{93.63} \\ \hline
\multirow{4}{*}{Test} & Accuracy & \textbf{96.98} & \textbf{98.32} \\
 & Precision & \textbf{94.14} & \textbf{91.73} \\
 & Recall & \textbf{94.19} & \textbf{92.22} \\
 & F1 & \textbf{93.87} & \textbf{91.63} \\ \hline
\end{tabular}
\captionof{table}{Results without dropout layers}
\label{tab:no_dropout}
\end{center}

\subsubsection{No CRF layer}
For the final set of experiments I have removed the CRF layer which left the
network with CNN and Bi-LSTM layers. While the results from the original paper
show only a slight performance decrease, my performance has dropped
substantially, especially on the NER task. Results are shown in table
\ref{tab:no_crf}

\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
& & {\textbf{POS}} & {\textbf{NER}}\\ \hline
\multirow{4}{*}{Development} & Accuracy & 77.63 & 51.89 \\
 & Precision & 84.80  & 63.25 \\
 & Recall & 83.89 & 53.90 \\
 & F1 & 82.42 & 49.09 \\ \hline
\multirow{4}{*}{Test} & Accuracy & 78.16 & 48.32 \\
 & Precision & 84.68 & 61.47 \\
 & Recall & 83.95 & 50.84 \\
 & F1 & 82.65 & 46.12 \\ \hline
\end{tabular}
\captionof{table}{Results without the CRF layer}
\label{tab:no_crf}
\end{center}


\label{comparison}
\subsection{Comparison with the original results}
All of the replicated results are similar to the original ones up to 1 $-$ 2
percentage points. In POS tagging task the difference is due to using only a subsection of
the whole corpus. NER task results vary because of different examples used
in training, development, and test sets.

Despite the slight differences, it is interesting to see that dropout had a
negative impact on the network performance in both domains. The phenomenon is 
explained in Section \ref{peculiar_dropout}

Regarding the adequate number of training epochs, my experiments have
demonstrated that the metrics scores stabilize at around the 10th epoch and
don't continue to improve from there. The proper way to address the issue would
be to use the early stopping technique, but I have relied on authors value of 50.
The reasoning behind this decision
is that if the results haven't improved for 40 epochs, they won't continue
improving afterwards which is practically the early stopping method.


\section{Experiments}

\subsection{Data Sets}
Data sets used in the paper were WSJ Treebank for POS tagging and CoNLL 2003
for the NER task. I have used only a portion of the Treebank because it isn't
the full version requires a special licence. The NLTK data package includes a
$10\%$ sample of the Treebank. NER data set was used in its full form.
Although WSJ and CoNLL have  pre-determined splits for the training,
development and test sets, I have decided to merge them and then do the custom
split. Data sets were split on a sentence level using the $60 - 20 - 20$ ratios
and random shuffling was applied before the split. Table \ref{tab:data} shows
the number of examples in each of splits.

\begin{center}
\begin{tabular}{ r|c|c| }
\multicolumn{1}{r}{}
 &  \multicolumn{1}{c}{WSJ Treebank NLTK sample}
 & \multicolumn{1}{c}{ConLL 2003 NER} \\
\cline{2-3}
Train & 2348 & 12446 \\
\cline{2-3}
Development & 783 & 4149 \\
\cline{2-3}
Test & 783 & 4149 \\
\cline{2-3}
\end{tabular}
\captionof{table}{Number of examples (sentences) in dataset splits}
\label{tab:data}
\end{center}


\textbf{WSJ Penn Treebank}
Data set is a consists of 2499 selected stories from a three year Wall Street
Journal (WSJ) collection of 98732 stories for syntactic annotation. There are
45 unique POS tags including the $\text{-NONE-}$ tag for words without known POS tag. 

\textbf{CoNLL 2003 NER}
The shared task of CoNLL 2003 concerns language-independent named entity
recognition. Four types of entities existing in the data set are: persons,
locations, organizations and names of miscellaneous entities that do not belong
to the previous three groups. My experiments were focused on the English
version of the data although German data is also available.

\subsection{Results}
All experiment used the same hyper parameters which are shown in Table
\ref{tab:hyper} If the hyper parameter is not present in the network
architecture e.g. Dropout in the Section \ref{no_dropout}, then the parameter
is simply ignored. 

\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
\textbf{Layer} & \textbf{Hyper Parameter} & \textbf{POS} & \textbf{NER} \\ \hline
\multirow{2}{*}{CNN} & window size & 3 & 3 \\
 & number of filters & 30 & 30 \\ \hline
\multirow{3}{*}{LSTM} & state size & 200 & 200 \\
 & initial state & 0.0 & 0.0 \\
 & peepholes & no & no \\ \hline
Dropout & dropout rate & 0.5 & 0.5 \\ \hline
\multirow{2}{*}{} & batch size & 10 & 10\\
 & initial learning rate & 0.01 & 0.015 \\
 & decay rate & 0.05 & 0.05 \\
 & training epochs & 50 & 50 \\
\hline
\end{tabular}
\captionof{table}{Per layer hyper parameters for datasets}
\label{tab:hyper}
\end{center}

\label{no_dropout}
\subsubsection{With Dropout}
First set of experiments were conducted with dropout layers. Results after 50
epochs of training are displayed in the Table \ref{tab:dropout}

\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
& {\textbf{POS}} & \multicolumn{2}{|c|}{\textbf{NER}}\\ \hline
\multirow{4}{*}{Development} & Accuracy & 96.73 & \textbf{98.80} \\
 & Precision & 93.77 & 93.73 \\
 & Recall & 93.91 & 94.02 \\
 & F1 & 93.52 & 93.56 \\ \hline
\multirow{4}{*}{Test} & Accuracy & 96.71 & 98.23 \\
 & Precision & 93.73 & 91.45 \\
 & Recall & 93.80 & 91.90 \\
 & F1 & 93.45 & 91.30 \\ \hline
\end{tabular}
\captionof{table}{Results with dropout layers}
\label{tab:dropout}
\end{center}

\label{peculiar_dropout}
\subsubsection{No Dropout}
Table \ref{tab:no_dropout} shows the results when dropout layers were removed.
In comparison with dropout layers we can see the slight improvement in all
metrics across both data sets. Dropout technique usually gives better results
on the unseen examples but the explanation could be that my data was shuffled
on a sentence level across the whole corpus whereas authors have used first $n$
articles/sections for training and the rest for development and testing. It
could be that the few last articles/sections differ slightly in context from
the rest, so the network with dropout applied would actually perform better. On
contrary, my model has seen examples from the last few articles/sections so it
performs better when there is no regularization applied in form of dropout.

\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
& {\textbf{POS}} & \multicolumn{2}{|c|}{\textbf{NER}}\\ \hline
\multirow{4}{*}{Development} & Accuracy & \textbf{97.08} & 98.78 \\
 & Precision & \textbf{94.24} & \textbf{93.69} \\
 & Recall & \textbf{94.28} & \textbf{94.13} \\
 & F1 & \textbf{93.97} & \textbf{93.63} \\ \hline
\multirow{4}{*}{Test} & Accuracy & \textbf{96.98} & \textbf{98.32} \\
 & Precision & \textbf{94.14} & \textbf{91.73} \\
 & Recall & \textbf{94.19} & \textbf{92.22} \\
 & F1 & \textbf{93.87} & \textbf{91.63} \\ \hline
\end{tabular}
\captionof{table}{Results without dropout layers}
\label{tab:no_dropout}
\end{center}


\subsection{Comparison with the original results}
All of the replicated results are similar to the original ones up to 1 $-$ 2
percentage points. In POS tagging task the difference is due to using only a subsection of
the whole corpus. NER task results vary because of different examples used
in training, development, and test sets.

Despite the slight differences, it is interesting to see that dropout had a
negative impact on the network performance in both domains. The phenomenon is 
explained in Section \ref{peculiar_dropout}

Regarding the adequate number of training epochs, my experiments have
demonstrated that the metrics scores stabilize at around the 10th epoch and
don't continue to improve from there. The proper way to address the issue would
be to use the early stopping technique\cite{early_stop}, but I have used
authors value of 50 as my reference point. The reasoning behind this decision
is that if the results haven't improved for 40 epochs, they won't continue
improving afterwards which is practically the definition of early stopping.

